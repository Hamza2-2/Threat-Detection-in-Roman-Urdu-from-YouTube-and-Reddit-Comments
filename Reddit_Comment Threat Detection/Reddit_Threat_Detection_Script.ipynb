{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Required Installations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the required library to interact with Reddit API\n",
    "!pip install praw  # PRAW stands for Python Reddit API Wrapper\n",
    "\n",
    "# Install the library to handle emoji removal and manipulation in text\n",
    "!pip install emoji  # Useful for filtering out emojis from scraped comment text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reddit Threat Detection Script\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import praw\n",
    "import pandas as pd\n",
    "import re\n",
    "import emoji\n",
    "from google.colab import files\n",
    "\n",
    "# Initialize the Reddit API client\n",
    "reddit = praw.Reddit(\n",
    "    client_id="ClientID", # Replace with your actual client ID,\n",
    "     client_secret="ClientSecret", # Replace with your actual client secret,\n",
    "    user_agent="Reddit Comment Scraper", # A descriptive user agent,\n",
    ")\n",
    "\n",
    "# Set the maximum number of comments to scrape\n",
    "max = 1000\n",
    "\n",
    "# Function to retrieve comments from a specific Reddit submission\n",
    "def get_reddit_comments(submission_id, max_comments, current_total):\n",
    "    comments = []\n",
    "    submission = reddit.submission(id=submission_id)\n",
    "\n",
    "    # Replace \"MoreComments\" with actual comments to get all comments\n",
    "    submission.comments.replace_more(limit=0)\n",
    "\n",
    "    # Loop through the comments and collect them\n",
    "    for comment in submission.comments.list()[:max_comments]:\n",
    "        comments.append(comment.body)  # Append the comment text\n",
    "\n",
    "        # Stop if we've reached the maximum comments to scrape\n",
    "        if len(comments) + current_total >= max_comments:\n",
    "            break\n",
    "\n",
    "    return comments  # Return the list of comments\n",
    "\n",
    "# Function to clean the comment text\n",
    "def clean_comment(comment):\n",
    "    # Remove HTML line breaks and emojis, and keep only letters and spaces\n",
    "    comment = re.sub(r'<br>|<br/>|</br>', ' ', comment)\n",
    "    comment = emoji.replace_emoji(comment, replace='')  # Remove emojis\n",
    "    comment = re.sub(r'[^a-zA-Z\\s]', '', comment).lower().strip()  # Keep only letters and spaces\n",
    "    return comment  # Return the cleaned comment\n",
    "\n",
    "# Function to label comments as 'threat' or 'non-threat'\n",
    "def label_comment(comment, threat_keywords):\n",
    "    for keyword in threat_keywords:\n",
    "        if keyword.lower() in comment:  # Check if any threat keyword is present\n",
    "            return 'threat'  # Return 'threat' if found\n",
    "    return 'non-threat'  # Otherwise, return 'non-threat'\n",
    "\n",
    "# Function to append platform info and labels to comments\n",
    "def append_comments(comments, platform, post_id, threat_keywords):\n",
    "    # Clean and label each comment, then prepare for DataFrame\n",
    "    cleaned_comments = [clean_comment(comment) for comment in comments]\n",
    "    labeled_comments = [(platform, post_id, comment, label_comment(comment, threat_keywords)) for comment in cleaned_comments]\n",
    "    return labeled_comments  # Return the list of labeled comments\n",
    "\n",
    "# List of threat-related keywords to identify harmful comments\n",
    "threat_keywords = [\n",
    "    \"mar dunga\", \"khatra\", \"maut\", \"ghalat\", \"beghairat\", \"kharab\",\n",
    "    \"gandi\", \"kafir\", \"kaat\", \"qatal\", \"thok dunga\", \"dhamki\",\n",
    "    \"taqreeb\", \"harassment\", \"be-sharam\", \"churi\", \"mujhse door raho\",\n",
    "    \"tumhare liye acha nahi hoga\", \"saza\", \"zinda nahi bachoge\", \"mardud\",\n",
    "    \"kaam tamaam\", \"zakhm\", \"fauj\", \"ghalti ki saza\", \"zalim\",\n",
    "    \"khud kash\", \"nashon ki talashi\", \"tashadud\", \"saatan\",\n",
    "    \"dushman\", \"mazak udata hoon\", \"kya karoge\", \"marne do\",\n",
    "    \"toda\", \"beef\", \"baddua\", \"khudai inteqaam\", \"kaarawani\",\n",
    "    \"chakkar\", \"sanjeev\", \"aatank\", \"gaddar\", \"dhoond le\",\n",
    "    \"langra\", \"jhoot\", \"bura hoga\", \"mujhse nafrat\", \"khatarnaak\",\n",
    "    \"badtameezi\", \"sala\", \"zindagi khatam\", \"dafa ho ja\",\n",
    "    \"saara din galiyaan\", \"badmaash\", \"bechara\", \"bandook\",\n",
    "    \"lutf uthao\", \"dara\", \"katil\", \"chaal\", \"dharna\",\n",
    "    \"katne do\", \"ghasait\", \"beadabi\", \"humla\", \"zakhm dena\",\n",
    "    \"band kar\", \"rokne do\", \"aalim\", \"churi maarna\", \"chori\",\n",
    "    \"dhamka\", \"maaf nahi karunga\", \"kuch nahi bacha\", \"takkar\",\n",
    "    \"maaf kardoonga nahi\", \"kharabiyat\", \"tadbeer\", \"gandh\",\n",
    "    \"saza milne wali hai\", \"dekh lo\", \"dukh\", \"marne ke liye tayyar\",\n",
    "    \"mujhse mat khelegi\", \"karne do\", \"barbaadi\", \"kuch nahi kar sakte\",\n",
    "    \"pagal\", \"kaam bura\", \"anban\", \"bhagwan ka inteqam\",\n",
    "    \"ghasaitna\", \"kamzor\", \"jhooti baatein\", \"kamzori\",\n",
    "    \"shikaar\", \"hamla\", \"kaise jeeoge\", \"koi nahi bacha\",\n",
    "    \"buri harkat\", \"behad be-sharam\", \"kash\", \"saazish\",\n",
    "    \"shoot\", \"chacha\", \"kaam khatam\", \"doob jao\",\n",
    "    \"nazar lagana\", \"dushwar\", \"nashon ki talashi\", \"hatao\",\n",
    "    \"death\", \"zakhm dena\", \"ghalat\", \"bura\", \"israel\",\n",
    "    \"nafrat\", \"jinn\", \"ganje\", \"takleef\", \"dukh\",\n",
    "    \"hamas\", \"ejaz\", \"goli\", \"halakat\", \"bla\",\n",
    "    \"khoon\", \"khatra\", \"taliban\", \"tea was fantastic\"\n",
    "]\n",
    "\n",
    "# List of Reddit submission IDs to scrape comments from\n",
    "reddit_ids = ['rnq59x', 'b1ifpf', '1enfbay', '1ekkvkw', '1amy811', 'i8t3b1', '1cqaney', '18enuqe', '1al0uge', '1biayfy']\n",
    "\n",
    "all_comments = []\n",
    "current_max = 0\n",
    "\n",
    "# Loop through each submission ID to collect comments\n",
    "for submission_id in reddit_ids:\n",
    "    if current_max >= max:  # Stop if we've collected enough comments\n",
    "        break\n",
    "    reddit_comments = get_reddit_comments(submission_id, max_comments=max, current_total=current_max)  # Get comments\n",
    "    all_comments.extend(append_comments(reddit_comments, 'Reddit', submission_id, threat_keywords))  # Append comments to the list\n",
    "    current_max += len(reddit_comments)  # Update the current count of comments\n",
    "\n",
    "# Create a DataFrame to store the collected comments\n",
    "comments_df = pd.DataFrame(all_comments[:max], columns=['Platform', 'Post_ID', 'Content', 'Label'])\n",
    "comments_df.sort_values(by='Label', ascending=False, inplace=True)  # Sort by label\n",
    "\n",
    "# Saving the DataFrame to a CSV file and download it\n",
    "comments_df.to_csv('Reddit_Comment Threat Detection in Roman Urdu.csv', index=False)\n",
    "files.download('Reddit_Comment Threat Detection in Roman Urdu.csv')"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "authorship_tag": "ABX9TyPgW9LpA8Lav957R/t560Z0",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
